---
phase: 03-ccr-integration
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/llm/config-generator.js
  - src/llm/agent.js
  - src/llm/prompts.js
  - src/index.js
autonomous: true

must_haves:
  truths:
    - "CCR config generated at ~/.claude-code-router/config.json with NON_INTERACTIVE_MODE"
    - "Multi-provider configuration supports OpenRouter, Anthropic, DeepSeek"
    - "API keys interpolated from environment variables using $VAR syntax"
    - "Agent SDK wrapper configured to route through CCR proxy"
  artifacts:
    - path: "src/llm/config-generator.js"
      provides: "CCR config file generator"
      exports: ["generateCCRConfig"]
      min_lines: 30
    - path: "src/llm/agent.js"
      provides: "Agent SDK wrapper with CCR proxy routing"
      exports: ["executeLLMTask", "executeLLMTaskWithRetry"]
      min_lines: 40
    - path: "src/llm/prompts.js"
      provides: "Prompt template helpers"
      exports: ["createMilestonePrompt"]
      min_lines: 10
  key_links:
    - from: "src/llm/config-generator.js"
      to: "~/.claude-code-router/config.json"
      via: "runtime file generation"
      pattern: "writeFileSync"
    - from: "src/llm/agent.js"
      to: "@anthropic-ai/claude-agent-sdk"
      via: "import { query }"
      pattern: "import.*query.*from.*claude-agent-sdk"
    - from: "src/llm/config-generator.js"
      to: "process.env"
      via: "environment variable read"
      pattern: "process\\.env\\.(OPENROUTER|ANTHROPIC|DEEPSEEK)_API_KEY"
---

<objective>
Create CCR config generation and Agent SDK integration layer for multi-provider LLM routing.

Purpose: Generate ~/.claude-code-router/config.json for CCR service with multi-provider support, and create Agent SDK wrapper that routes through CCR proxy. Config generator creates service configuration, Agent SDK wrapper provides clean API for command handlers.

Output: Config generator, Agent SDK wrapper, prompt templates, ready for command execution in later phases.

**Architecture**: CCR service (started in workflow) reads config from ~/.claude-code-router/config.json. Agent SDK wrapper connects to CCR via ANTHROPIC_BASE_URL=http://127.0.0.1:3456.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-ccr-integration/03-RESEARCH.md
@.planning/phases/03-ccr-integration/03-RESEARCH-CCR.md

# Prior plan context

@.planning/phases/03-ccr-integration/03-01-SUMMARY.md

# Files to modify

@src/index.js
</context>

<tasks>

<task type="auto">
  <name>Create CCR config generation module</name>
  <files>src/llm/config-generator.js</files>
  <action>
Create `src/llm/config-generator.js` to generate CCR service config at `~/.claude-code-router/config.json`:

**Function: generateCCRConfig()**

- Reads API keys from `process.env.OPENROUTER_API_KEY`, `process.env.ANTHROPIC_API_KEY`, `process.env.DEEPSEEK_API_KEY`
- Creates `~/.claude-code-router/` directory if missing (use `mkdirSync` with recursive: true)
- Generates config.json with CCR service configuration per 03-RESEARCH-CCR.md schema
- Uses `$VAR_NAME` syntax for env var interpolation (CCR replaces these at runtime)
- Sets `NON_INTERACTIVE_MODE: true` (CRITICAL for CI - prevents prompts/hangs)
- Configures multi-provider routing with priorities

**Config structure** (based on CCR service schema from RESEARCH-CCR.md):

```json
{
  "NON_INTERACTIVE_MODE": true,
  "LOG": true,
  "LOG_LEVEL": "info",
  "API_TIMEOUT_MS": 600000,
  "Providers": [
    {
      "name": "openrouter",
      "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
      "api_key": "$OPENROUTER_API_KEY",
      "models": ["anthropic/claude-sonnet-4", "google/gemini-2.5-pro-preview"],
      "transformer": { "use": ["openrouter"] }
    },
    {
      "name": "anthropic",
      "api_base_url": "https://api.anthropic.com/v1",
      "api_key": "$ANTHROPIC_API_KEY",
      "models": ["claude-sonnet-4-5-20250929"],
      "transformer": { "use": ["anthropic"] }
    },
    {
      "name": "deepseek",
      "api_base_url": "https://api.deepseek.com/chat/completions",
      "api_key": "$DEEPSEEK_API_KEY",
      "models": ["deepseek-chat", "deepseek-reasoner"],
      "transformer": { "use": ["deepseek"] }
    }
  ],
  "Router": {
    "default": "openrouter,anthropic/claude-sonnet-4",
    "think": "deepseek,deepseek-reasoner",
    "longContext": "openrouter,google/gemini-2.5-pro-preview"
  }
}
```

**Implementation notes**:

- Only include providers with env vars defined (skip if `process.env.X_API_KEY` is undefined)
- If no API keys available, throw error: "At least one API key required: OPENROUTER_API_KEY, ANTHROPIC_API_KEY, or DEEPSEEK_API_KEY"
- Use `path.join(os.homedir(), '.claude-code-router', 'config.json')` for config path
- Write with `fs.writeFileSync` (synchronous OK for startup config)
- Use `$VAR_NAME` syntax (not actual values) - CCR interpolates at service start

**Why NON_INTERACTIVE_MODE**: Per 03-RESEARCH-CCR.md, this is MANDATORY for CI/CD. Sets `CI=true`, `FORCE_COLOR=0`, configures stdin to prevent hangs.

**Why $VAR_NAME syntax**: CCR service interpolates environment variables at startup. Workflow sets env vars, CCR reads them when service starts.

**API key flow**: GitHub Secrets → Workflow env vars → CCR service reads $VAR_NAME from config → Replaces with actual values
</action>
<verify>

```bash
# Verify module exists
test -f src/llm/config-generator.js && echo "Config generator created"

# Verify function exports
grep -q "export.*generateCCRConfig" src/llm/config-generator.js && echo "Function exported"

# Verify CCR service config structure
grep -q '"NON_INTERACTIVE_MODE"' src/llm/config-generator.js && echo "Non-interactive mode configured"
grep -q '"Providers"' src/llm/config-generator.js && echo "Multi-provider support"
grep -q '"Router"' src/llm/config-generator.js && echo "Routing configuration"

# Verify env var interpolation syntax (uses $VAR_NAME, not process.env)
grep -q '\$OPENROUTER_API_KEY\|\$ANTHROPIC_API_KEY\|\$DEEPSEEK_API_KEY' src/llm/config-generator.js && echo "Env var syntax present"

# Verify file I/O
grep -q "writeFileSync\|writeFile" src/llm/config-generator.js && echo "Config file writing present"
```

  </verify>
  <done>src/llm/config-generator.js exists with generateCCRConfig function, creates ~/.claude-code-router/config.json with NON_INTERACTIVE_MODE, Providers array, Router config, uses $VAR_NAME syntax for env var interpolation</done>
</task>

<task type="auto">
  <name>Create Agent SDK wrapper module</name>
  <files>src/llm/agent.js</files>
  <action>
Create `src/llm/agent.js` with Agent SDK wrapper based on patterns from 03-RESEARCH.md:

**Function 1: executeLLMTask(prompt, options)**

- Import `query` from `@anthropic-ai/claude-agent-sdk`
- Set default options for CI environment:
  - `model: "claude-sonnet-4-5-20250929"` (pin specific model)
  - `permissionMode: "acceptEdits"` (auto-approve file edits in CI, no prompts)
  - `allowedTools: ["Read", "Write", "Bash", "Glob", "Grep"]` (enable file and command tools)
  - `maxTurns: 50` (reasonable limit for command execution)
- Process message stream asynchronously using `for await`
- Collect text blocks from assistant messages
- Return `{ success: true, output: messages.join("\n") }` on success
- Catch errors and return `{ success: false, error: error.message }`

**Function 2: executeLLMTaskWithRetry(prompt, options, maxRetries = 3)**

- Wrapper around executeLLMTask with exponential backoff
- Retry on rate limit (429) or overloaded (529) errors
- Don't retry on authentication errors (401)
- Detect errors in message content (SDK may return errors as text, not exceptions)
- Exponential backoff: 2^attempt seconds between retries

**CCR proxy connection**:
Agent SDK automatically routes through CCR when `ANTHROPIC_BASE_URL` environment variable is set to `http://127.0.0.1:3456`. No explicit CCR invocation needed - SDK handles HTTP routing.

**Example implementation**:

```javascript
import { query } from "@anthropic-ai/claude-agent-sdk";

export async function executeLLMTask(prompt, options = {}) {
  try {
    const messages = [];

    // Agent SDK automatically uses ANTHROPIC_BASE_URL if set
    // Routes to CCR proxy at http://127.0.0.1:3456
    for await (const msg of query({
      prompt,
      options: {
        model: options.model || "claude-sonnet-4-5-20250929",
        permissionMode: "acceptEdits", // Non-interactive
        allowedTools: options.allowedTools || [
          "Read",
          "Write",
          "Bash",
          "Glob",
          "Grep",
        ],
        maxTurns: options.maxTurns || 50,
        ...options,
      },
    })) {
      if (msg.type === "assistant") {
        messages.push(msg.message.content);
      }
    }

    return { success: true, output: messages.join("\n") };
  } catch (error) {
    return { success: false, error: error.message };
  }
}
```

**Why permissionMode: acceptEdits**: Default SDK behavior is interactive prompts for file changes. In CI, there's no TTY, so we must auto-approve. This satisfies CCR-07 (non-interactive mode).

**Why no explicit CCR call**: Agent SDK is HTTP client. When ANTHROPIC_BASE_URL points to CCR proxy (http://127.0.0.1:3456), SDK sends requests there instead of Anthropic API. CCR intercepts, routes to configured provider.

**Reference**: 03-RESEARCH.md sections "Pattern 1: Agent SDK Initialization", "Code Examples: Basic Agent SDK Usage in GitHub Action", and "Error Handling Pattern".
</action>
<verify>

```bash
# Verify module exists and has correct exports
test -f src/llm/agent.js && echo "Module created"
grep -q "export.*executeLLMTask" src/llm/agent.js && echo "Main function exported"
grep -q "export.*executeLLMTaskWithRetry" src/llm/agent.js && echo "Retry function exported"

# Verify SDK import
grep -q "import.*query.*from.*claude-agent-sdk" src/llm/agent.js && echo "SDK imported"

# Verify CI-safe configuration
grep -q "permissionMode.*acceptEdits" src/llm/agent.js && echo "Non-interactive mode configured"
grep -q "allowedTools" src/llm/agent.js && echo "Tools configured"

# Verify error handling
grep -q "try.*catch" src/llm/agent.js && echo "Error handling present"
```

  </verify>
  <done>src/llm/agent.js exists with executeLLMTask and executeLLMTaskWithRetry exports, imports Agent SDK query function, configures permissionMode: acceptEdits, includes error handling and retry logic, routes through CCR via ANTHROPIC_BASE_URL</done>
</task>

<task type="auto">
  <name>Create prompt template helpers</name>
  <files>src/llm/prompts.js</files>
  <action>
Create `src/llm/prompts.js` with prompt template functions for GSD commands:

**Function: createMilestonePrompt(args)**

- Accepts args object with milestone specification
- Returns formatted prompt string for milestone creation
- Template should include:
  - Clear instruction to create milestone planning documents
  - Specification data (JSON.stringify(args, null, 2))
  - Instruction to review codebase before creating files
  - Expected outputs: PROJECT.md, ROADMAP.md, STATE.md in .github/planning/

Example structure:

```javascript
export function createMilestonePrompt(args) {
  return `
Create a new GSD milestone based on this specification:
${JSON.stringify(args, null, 2)}

Instructions:
1. Review the existing codebase structure
2. Create planning documents in .github/planning/:
   - PROJECT.md with milestone context
   - ROADMAP.md with phase structure
   - STATE.md with milestone status
3. Follow GSD planning standards

Respond with file contents and any questions needed for requirements gathering.
  `.trim();
}
```

**Why template functions**: Separates prompt engineering from execution logic. Makes prompts testable and reusable across command handlers. Easier to iterate on prompt quality without touching agent wrapper.

**Note**: Additional prompt templates (for other GSD commands) will be added in later phases (Phase 5+). This plan creates the module and adds the first template.
</action>
<verify>

```bash
# Verify module exists and exports
test -f src/llm/prompts.js && echo "Module created"
grep -q "export.*createMilestonePrompt" src/llm/prompts.js && echo "Template function exported"

# Verify prompt structure
grep -q "JSON.stringify" src/llm/prompts.js && echo "Prompt includes args serialization"
grep -q "planning" src/llm/prompts.js && echo "Prompt mentions planning docs"
```

  </verify>
  <done>src/llm/prompts.js exists with createMilestonePrompt export, template includes milestone spec and planning doc instructions</done>
</task>

<task type="auto">
  <name>Add LLM module placeholder integration</name>
  <files>src/index.js</files>
  <action>
Update `src/index.js` to import the LLM wrapper (but don't execute yet - that's for Phase 5):

Add import at top of file:

```javascript
import { executeLLMTaskWithRetry } from "./llm/agent.js";
import { createMilestonePrompt } from "./llm/prompts.js";
```

Add comment in the TODO section (line 53, after config loading):

```javascript
// TODO: Execute command logic in later phases (Phase 4+)
// LLM integration ready - executeLLMTaskWithRetry and createMilestonePrompt available
// CCR proxy service must be running with ANTHROPIC_BASE_URL=http://127.0.0.1:3456
// Config generated via generateCCRConfig() in workflow before service start
// Example: const result = await executeLLMTaskWithRetry(createMilestonePrompt(sanitizedArgs));
```

**Why add imports now**: Validates that modules are correctly structured and importable. Ensures no circular dependency issues. Provides clear hook point for Phase 5 command execution.

**Why not execute yet**: Phase 3 focuses on LLM integration infrastructure. Phase 5 (Milestone Creation Workflow) will implement the actual command execution logic. This task proves the integration is ready without changing behavior.

**Note**: Do NOT add config generation call here. Config generation will be called in workflow (Plan 03-03), not in action code.
</action>
<verify>

```bash
# Verify imports added
grep -q "import.*executeLLMTaskWithRetry.*from.*llm/agent" src/index.js && echo "Agent wrapper imported"
grep -q "import.*createMilestonePrompt.*from.*llm/prompts" src/index.js && echo "Prompts imported"

# Verify TODO comment updated
grep -q "LLM integration ready" src/index.js && echo "Integration placeholder added"
grep -q "CCR proxy service" src/index.js && echo "CCR proxy mentioned"
grep -q "ANTHROPIC_BASE_URL" src/index.js && echo "Base URL mentioned"

# Verify no execution yet (should still have original TODO line)
grep -q "TODO: Execute command logic in later phases" src/index.js && echo "Execution deferred to Phase 5"

# Verify code still runs (basic syntax check)
node --check src/index.js && echo "Syntax valid"
```

  </verify>
  <done>src/index.js imports LLM wrapper and prompt functions, includes integration placeholder comment with CCR proxy details, no execution logic added yet, syntax validation passes</done>
</task>

</tasks>

<verification>
Run all task verification commands. Success criteria:
- src/llm/config-generator.js exists with generateCCRConfig export
- Config generation creates ~/.claude-code-router/config.json with NON_INTERACTIVE_MODE, Providers, Router
- Uses $VAR_NAME syntax for env var interpolation (CCR interpolates at runtime)
- src/llm/agent.js exists with executeLLMTask and executeLLMTaskWithRetry exports
- Agent wrapper imports SDK query function and configures permissionMode: acceptEdits
- src/llm/prompts.js exists with createMilestonePrompt export
- src/index.js imports LLM modules without executing them
- All files pass Node.js syntax validation (node --check)
</verification>

<success_criteria>
**Requirements satisfied**: CCR-03 (config generated at runtime), CCR-04 (API keys via $VAR interpolation), CCR-05 (SDK routes through CCR), CCR-07 (NON_INTERACTIVE_MODE + permissionMode)

**Observable behaviors**:

1. Config generator creates CCR service config at ~/.claude-code-router/config.json
2. Multi-provider configuration with OpenRouter, Anthropic, DeepSeek
3. Uses $VAR_NAME syntax for CCR's env var interpolation
4. NON_INTERACTIVE_MODE set to true (prevents CI hangs)
5. Agent SDK wrapper routes through CCR via ANTHROPIC_BASE_URL
6. permissionMode: acceptEdits for non-interactive execution

**Files created**:

- src/llm/config-generator.js: CCR service config generator
- src/llm/agent.js: Agent SDK wrapper with retry logic
- src/llm/prompts.js: Prompt template helpers

**Files modified**:

- src/index.js: Added LLM module imports

**Ready for next plan**: CCR config generator ready to be called in workflow (Plan 03-03). Agent SDK wrapper ready for command execution via CCR proxy.

**Architecture note**: Workflow will: 1) Call generateCCRConfig() to create config file, 2) Install CCR globally, 3) Start CCR service, 4) Set ANTHROPIC_BASE_URL=http://127.0.0.1:3456, 5) Use Agent SDK wrapper which routes through CCR.
</success_criteria>

<output>
After completion, create `.planning/phases/03-ccr-integration/03-02-SUMMARY.md` with:
- Performance metrics
- Module structure and exports
- CCR service config format (NON_INTERACTIVE_MODE, Providers, Router)
- Env var interpolation pattern ($VAR_NAME syntax)
- Agent SDK proxy routing mechanism (ANTHROPIC_BASE_URL)
- Any deviations from plan
- Integration verification results
</output>
